### **03.Federated Learning Workflow**

| 
**Step**

 | 

**Description**

 | 

**Key Elements**

 |
| --- | --- | --- |
| 

**1\. Local Training**

 | 

Each client (device or node) trains its own local model using its private data. Training hyperparameters may vary among clients.

 | 

\- **Decentralized Data:** Data remains on local devices, ensuring privacy and security.  
\- **Data Distribution:**  
1) **IID**: Clients have similar data distributions.  
2) **Non-IID**: Clients have different data distributions, which can vary in feature space, label space, or both.  
\- **Learning Model:** Trained on decentralized data, improving generalization.  
\- **Clients:** Devices or nodes participating in FL.

 |
| 

**2\. Communication**

 | 

Model updates are transmitted from local clients to a central server or distributed network. Communication plays a role in coordinating updates and ensuring data privacy.

 | 

\- **Communication Schedule:** Can be **synchronous** (all clients update together) or **asynchronous** (clients update independently).  
\- **Privacy Protocols:** Protect model updates from privacy leaks and attacks using techniques like Differential Privacy (DP) or Secure Multi-Party Computation (SMC).

 |
| 

**3\. Aggregation**

 | 

The central server or network combines local model updates to create a global model using aggregation algorithms.

 | 

\- **Aggregation Mechanism:** Typically **Federated Averaging (FedAvg)**, but task-specific aggregators exist (e.g., clustering tasks require custom aggregation).

 |
| 

**4\. Local Update**

 | 

The global model is distributed back to local clients, either replacing or merging with their previous models.

 | 

\- **Update Strategies:** Clients may fully replace or partially merge the new model with their local model for personalization.

 |